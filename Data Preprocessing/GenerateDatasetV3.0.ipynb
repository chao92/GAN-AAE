{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Lebel edges and save into 3 files:\n",
    "- fully_shared_labels: two vertices of a given edge have totaly same labels \n",
    "- partial_shared_labels: two vertices share partial labels, and I use overlap ratio to indicate similarity\n",
    "- others: otherwise\n",
    "\n",
    "Sample edges:\n",
    "\n",
    "first randomly edges from fully_shared_labels , then if it is still not enough , select from partial_shared_labels, then others.\n",
    "\n",
    "Notes:\n",
    "- when select edges from fully_shared_labels and others, the method is random selection.\n",
    "- but when select edges from partial_shared_labels, the method is sequential selection, because edges have already been storted to descending orders by overlap values(similarities).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading groups from youtube-groups.txt\n",
      "vertex num:22693    label: num47\n",
      "valid labels: {19, 20, 6}\n",
      "removed 13831 vertex, remain 8862 vertex\n",
      "reading network from youtube.txt\n",
      "original #vertices and #edges: 22693 96361\n",
      "\n",
      "#fully:9489\n",
      "#partial:9697\n",
      "#others:4907\n",
      "#total edges:24093\n",
      "#vertices:7147\n",
      "using pre-setted nums\n",
      "The consitutuion of sampled dataset: \n",
      " Full:2000 Partial:3200 Others: 4800 Total:24093 \n",
      "others:10000\n",
      "####Done####\n"
     ]
    }
   ],
   "source": [
    "# data_path: working path\n",
    "# sampled_path : the generated files will be saved here\n",
    "# file_name : [blogCatalog3.txt,flickr.txt,youtube.txt]\n",
    "# group_file_name: for example [flickr-groups.txt] \n",
    "from collections import defaultdict\n",
    "def label_edges(data_path,sampled_path,file_name,group_file_name,keep_num=-1,max_label_classes=-1,full_num=-1,partial_num=-1,other_num=-1,random_seed=7766):\n",
    "    \n",
    "    \n",
    "    vertices = set()\n",
    "    edges = set()\n",
    "    groups = dict()\n",
    "    label_counter = defaultdict(int)\n",
    "    # first read labels from data_path+file_name\n",
    "    print('reading groups from %s' % group_file_name)\n",
    "    with open(data_path+'/'+group_file_name,'r+') as fin:\n",
    "        lines = fin.readlines()\n",
    "        items = lines[0].strip('\\n').split(' ')\n",
    "        num_of_vertex = int(items[0])\n",
    "        num_of_label = int(items[1])\n",
    "        print('vertex num:%d    label: num%d' % (num_of_vertex, num_of_label))\n",
    "        for line in lines[1:]:\n",
    "            items = line.strip('\\n').split(':')\n",
    "            # node is vertex\n",
    "            node = int(items[0].replace(' ',''))\n",
    "            assert len(items) == 2\n",
    "            if (len(items[1]) == 0):\n",
    "                groups[node] = set([-1]) \n",
    "            else:\n",
    "                id_strs = items[1].strip(' ')\n",
    "                groups[node] = set([int(x) for x in id_strs.split(' ')])\n",
    "                for idx in groups[node]:\n",
    "                  if idx != -1:\n",
    "                      label_counter[idx] += 1\n",
    "    \n",
    "    # sort label frequency\n",
    "    sorted_labels = sorted(label_counter.items(), key=lambda x:x[1],reverse=True)\n",
    "          \n",
    "    valid_label_ids = set()\n",
    "    if max_label_classes == -1:\n",
    "        max_label_classes = len(sorted_labels)\n",
    "    \n",
    "    for i in range(max_label_classes):\n",
    "        valid_label_ids.add(sorted_labels[i][0])\n",
    "    \n",
    "    # remove unvalid labels\n",
    "    removed_count = 0\n",
    "    cleaned_groups = dict()\n",
    "    for node in groups.keys():\n",
    "        labels = set(groups[node])\n",
    "        c_labels = labels & valid_label_ids\n",
    "        if len(c_labels) > 0:\n",
    "            cleaned_groups[node] = c_labels\n",
    "        else:\n",
    "            removed_count += 1\n",
    "    print('valid labels: %s' % valid_label_ids )\n",
    "    print('removed %d vertex, remain %d vertex' % (removed_count, len(cleaned_groups)))\n",
    "    groups = cleaned_groups\n",
    "    num_of_label = len(valid_label_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3 types of edges\n",
    "    fully_shared_labels = set() # \n",
    "    partial_shared_labels = set() #\n",
    "    others = set()\n",
    "    used_vertices = set()\n",
    "    \n",
    "    # then read network from data_path+file_name\n",
    "    print('reading network from %s' % file_name)\n",
    "    with open(data_path+'/'+file_name,'r+') as fin:\n",
    "        lines = fin.readlines()\n",
    "        print('original #vertices and #edges: '+ lines[0])\n",
    "        items = lines[0].strip('\\n').split(' ')\n",
    "        \n",
    "        for line in lines[1:]:\n",
    "            items = line.strip('\\n').split(' ')\n",
    "            node1 = int(items[0])\n",
    "            node2 = int(items[1])\n",
    "            if node1 not in cleaned_groups or node2 not in cleaned_groups:\n",
    "                continue\n",
    "            used_vertices.add(node1)\n",
    "            used_vertices.add(node2)\n",
    "            node1_label = groups[node1]\n",
    "            node2_label = groups[node2]\n",
    "            # overlap_ratio could used to sort\n",
    "            overlap_ratio = len(node1_label & node2_label) / len(node1_label | node2_label)\n",
    "            node1_str = ' '.join(list([ str(x) for x in node1_label]))\n",
    "            node2_str = ' '.join(list([ str(x) for x in node2_label]))\n",
    "            vertices.add(node1)\n",
    "            vertices.add(node2)\n",
    "\n",
    "            if (node2,node1) not in edges:\n",
    "                edges.add((node1,node2))\n",
    "            if (node1_label == node2_label) and (node1_label != set([-1])):\n",
    "                fully_shared_labels.add((node1,node2,1.0,node1_str,node2_str,overlap_ratio))\n",
    "            elif len(node1_label & node2_label) > 0 and (node1_label != set([-1])) and (node2_label != set([-1])):\n",
    "                partial_shared_labels.add((node1,node2,1.0,node1_str,node2_str,overlap_ratio))\n",
    "            else:\n",
    "                others.add((node1,node2,1.0,node1_str,node2_str,overlap_ratio))\n",
    "                \n",
    "        num_of_vertex = len(vertices) == num_of_vertex\n",
    "        num_of_edges=  len(edges)\n",
    "    edges = list(edges)\n",
    "\n",
    "    \n",
    "    partial_shared_labels = list(sorted(list(partial_shared_labels), key = lambda x:x[5], reverse =True))\n",
    "    \n",
    "    # periordically save files\n",
    "    file_map = {\n",
    "        'fully' : fully_shared_labels,\n",
    "        'partial': partial_shared_labels,\n",
    "        'others': others,\n",
    "    }\n",
    "    \n",
    "    fully_shared_labels = list(fully_shared_labels) # \n",
    "    partial_shared_labels = list(partial_shared_labels) #\n",
    "    others = list(others)\n",
    "    \n",
    "    total_edges = 0\n",
    "    \n",
    "    for file in file_map:\n",
    "        out_path = '%s/%s.labeled.%s' % (sampled_path,file_name,file)\n",
    "        with open(out_path, 'w+', encoding='utf-8') as fout:\n",
    "            container = file_map[file]\n",
    "            fout.write('%d\\n' % (len(container)))\n",
    "            print('#%s:%d' % (file,len(container)))\n",
    "            total_edges += len(container)\n",
    "            for edge in container:\n",
    "                fout.write('%s\\t%s\\t%.2f\\t%s\\t%s\\t%f\\n' % edge)\n",
    "    print('#%s:%d' % ('total edges', total_edges))\n",
    "    print('#vertices:%d' % len(used_vertices))\n",
    "    \n",
    "    # sample\n",
    "    import random\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    if keep_num == -1:\n",
    "        keep_num = total_edges\n",
    "    assert keep_num <= total_edges, 'the keep num should less than or equal to total edges'\n",
    "    if full_num != -1 and partial_num != -1 and other_num != -1:\n",
    "        print('using pre-setted nums')\n",
    "        nums_from_full = full_num\n",
    "        nums_from_partial = partial_num\n",
    "        nums_from_others = other_num\n",
    "    else:\n",
    "        nums_from_full = min(len(fully_shared_labels), keep_num)\n",
    "        nums_from_partial = min(len(partial_shared_labels), keep_num - nums_from_full)\n",
    "        nums_from_others = keep_num - nums_from_full - nums_from_partial\n",
    "    \n",
    "    res_set = []\n",
    "    if nums_from_full > 0 :\n",
    "        res_set += random.sample(fully_shared_labels, nums_from_full)\n",
    "    if nums_from_partial > 0:\n",
    "        res_set +=partial_shared_labels[0:nums_from_partial]\n",
    "    if nums_from_others > 0:\n",
    "        res_set += random.sample(others, nums_from_others)\n",
    "    \n",
    "    print('The consitutuion of sampled dataset: \\n Full:%d Partial:%d Others: %d Total:%d ' % (nums_from_full,nums_from_partial, nums_from_others, keep_num))\n",
    "   \n",
    "    \n",
    "    # write to file\n",
    "    out_path = '%s/%s.labeled.%s' % (sampled_path,file_name,'sampled')\n",
    "    with open(out_path, 'w+', encoding='utf-8') as fout:\n",
    "        container = res_set\n",
    "        fout.write('%d\\n' % (len(container)))\n",
    "        print('%s:%d' % (file,len(container)))\n",
    "        for edge in container:\n",
    "            fout.write('%s\\t%s\\t%.2f\\t%s\\t%s\\t%f\\n' % edge)\n",
    "    print('####Done####')\n",
    "\n",
    "#label_edges('raw_data','generated_data','blogCatalog.txt','blogCatalog-groups.txt',37500,max_label_classes=3)  \n",
    "label_edges('raw_data','generated_data','youtube.txt','youtube-groups.txt',full_num=2000,partial_num=3200,other_num=4800,max_label_classes=3) \n",
    "#label_edges('raw_data','generated_data','flickr.txt','flickr-groups.txt',full_num=11000,partial_num=11000,other_num=33000,max_label_classes=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 Assign new vertex id and groud id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def reindex(sampled_path,file_name):\n",
    "    fully_shared_labels = set() # \n",
    "    partial_shared_labels = set() #\n",
    "    others = set()\n",
    "    \n",
    "    new_vertex_index = dict()\n",
    "    new_label_index = dict()\n",
    "    \n",
    "    new_label_index[-1] = 0\n",
    "    \n",
    "    in_path = '%s/%s.labeled.%s' % (sampled_path,file_name,'sampled')\n",
    "    out_path = '%s/%s.labeled.%s' % (sampled_path,file_name,'reindex')\n",
    "    with open(in_path, 'r', encoding='utf-8') as fin:\n",
    "        with open(out_path, 'w+', encoding='utf-8') as fout:\n",
    "            lines = fin.readlines()\n",
    "            num = int(lines[0][0:-1])\n",
    "            for line in lines[1:]:\n",
    "                items = line.strip('\\n').split('\\t')\n",
    "                \n",
    "                vertex1 = str(items[0])\n",
    "                vertex2 = str(items[1])\n",
    "                weight = float(items[2])\n",
    "                labels1 = [int(x) for x in items[3].split(' ')]\n",
    "                labels2 = [int(x) for x in items[4].split(' ')]\n",
    "                overlap_ratio = float(items[5])\n",
    "\n",
    "                if vertex1 not in new_vertex_index:\n",
    "                    new_vertex_index[vertex1] = len(new_vertex_index)\n",
    "                if vertex2 not in new_vertex_index:\n",
    "                    new_vertex_index[vertex2] = len(new_vertex_index)\n",
    "\n",
    "                new_vertex1 = new_vertex_index[vertex1]\n",
    "                new_vertex2 = new_vertex_index[vertex2]\n",
    "\n",
    "                for label in labels1+labels2:\n",
    "                    if label not in new_label_index:\n",
    "                        new_label_index[label] = len(new_label_index)\n",
    "\n",
    "                new_labels1_str = ' '.join([str(new_label_index[x]) for x in labels1])\n",
    "                new_labels2_str = ' '.join([str(new_label_index[x]) for x in labels2])\n",
    "\n",
    "                new_edge = (new_vertex1,new_vertex2,weight,new_labels1_str,new_labels2_str,overlap_ratio)\n",
    "                fout.write('%s\\t%s\\t%.2f\\t%s\\t%s\\t%f\\n' % new_edge)\n",
    "\n",
    "    \n",
    "    out_path = '%s/%s.labeled.%s' % (sampled_path,file_name,'vertex_dict')\n",
    "\n",
    "    # old_vertex_id new_vertex_id\n",
    "    with open(out_path, 'w+', encoding='utf-8') as fout:\n",
    "        for key in new_vertex_index.keys():\n",
    "            fout.write('%s\\t%d\\n' % (key,new_vertex_index[key]))\n",
    "\n",
    "    # old_group_id new_group_id\n",
    "    out_path = '%s/%s.labeled.%s' % (sampled_path,file_name,'label_dict')\n",
    "    with open(out_path, 'w+', encoding='utf-8') as fout:\n",
    "        for key in new_label_index.keys():\n",
    "            fout.write('%d\\t%d\\n' % (key,new_label_index[key]))\n",
    "\n",
    "#reindex('generated_data','eco_flickr.txt')\n",
    "reindex('generated_data','eco_youtube.txt')\n",
    "#reindex('generated_data','eco_blogCatalog.txt')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Generate Input for GAN-AAE\n",
    "foramt:\n",
    "vertex1 vertex2\n",
    "\n",
    "### Input\n",
    "Your input graph data should be a **txt** file and be under **GraphData folder** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def generate_input(data_path,file_name):\n",
    "    GRAPH_PATH=data_path+'/output'\n",
    "    if os.path.exists(GRAPH_PATH) is False:\n",
    "        os.makedirs(GRAPH_PATH)\n",
    "    in_path = '%s/%s.labeled.%s' % (data_path,file_name,'vertex_dict')\n",
    "    with open(in_path,'r+',encoding='utf-8') as fin:\n",
    "        vertex_num = len(fin.readlines())\n",
    "        \n",
    "    in_path = '%s/%s.labeled.%s' % (data_path,file_name,'reindex')\n",
    "    with open(in_path,'r+',encoding='utf-8') as fin:\n",
    "        edge_num = len(fin.readlines())\n",
    "    \n",
    "    in_path = '%s/%s.labeled.%s' % (data_path,file_name,'label_dict')\n",
    "    with open(in_path,'r+',encoding='utf-8') as fin:\n",
    "        label_num = len(fin.readlines())\n",
    "    \n",
    "    \n",
    "    in_path = '%s/%s.labeled.%s' % (data_path,file_name,'reindex')\n",
    "    out_path = '%s/%s.data' % (GRAPH_PATH,file_name)\n",
    "    \n",
    "    new_group_infor = dict()\n",
    "    with open(in_path,'r+',encoding='utf-8') as fin:\n",
    "        with open(out_path,'w+',encoding='utf-8') as fout:\n",
    "            fout.write('%d %d\\n' % (vertex_num, edge_num))\n",
    "            lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                items = line.strip('\\n').split('\\t')\n",
    "                vertex1 = int(items[0])\n",
    "                vertex2 = int(items[1])\n",
    "                weight = float(items[2])\n",
    "                new_group_infor[vertex1] = items[3]\n",
    "                new_group_infor[vertex2] = items[4]\n",
    "                labels1 = [int(x) for x in items[3].split(' ')]\n",
    "                labels2 = [int(x) for x in items[4].split(' ')]\n",
    "                overlap_ratio = float(items[5])\n",
    "                fout.write('%d %d\\n' % (vertex1,vertex2))\n",
    "    out_path = '%s/%s.data_group' % (GRAPH_PATH,file_name)\n",
    "    with open(out_path,'w+',encoding='utf-8') as fout:\n",
    "        fout.write('%d %d\\n' % (vertex_num, label_num))\n",
    "        for key in sorted(new_group_infor.keys()):\n",
    "            fout.write('%d : %s\\n' % (key, new_group_infor[key]))\n",
    "\n",
    "\n",
    "#generate_input('generated_data','eco_blogCatalog.txt')\n",
    "#print(\"blog done\\n\")\n",
    "#generate_input('generated_data','eco_flickr.txt')\n",
    "#print(\"flickr done\\n\")\n",
    "generate_input('generated_data','eco_youtube.txt')\n",
    "print(\"youtube done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
